{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Transformers Grok Succinct Algorithms?\n",
    "\n",
    "## Interactive Analysis Notebook\n",
    "\n",
    "This notebook provides an interactive walkthrough of the experiments from the ACL 2026 paper:\n",
    "\n",
    "> **\"Do Transformers Grok Succinct Algorithms? Mechanistic Evidence for Counting Circuits\"**\n",
    "\n",
    "### Contents\n",
    "1. [Setup and Data Exploration](#1-setup)\n",
    "2. [Transformer Training with Grokking](#2-training)\n",
    "3. [Mechanistic Analysis](#3-analysis)\n",
    "4. [Visualization](#4-visualization)\n",
    "5. [RNN Baseline Comparison](#5-baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Exploration <a id='1-setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install torch numpy matplotlib seaborn tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.data.counter_dataset import (\n",
    "    LargeCounterDataset, \n",
    "    create_dataloader,\n",
    "    int_to_binary,\n",
    "    get_carry_chain_length\n",
    ")\n",
    "from src.models.transformer import TransformerConfig, TransformerLM\n",
    "from src.models.rnn import RNNConfig, RNNLM\n",
    "from src.training.trainer import TrainingConfig, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understanding the LARGECOUNTER Task\n",
    "\n",
    "The task is to predict the next n-bit binary number:\n",
    "- Input: `N_i # ` (current number followed by delimiter)\n",
    "- Output: `N_{i+1}` (next number in sequence)\n",
    "\n",
    "Example: `0111 # 1000` (7 → 8 in binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate binary counting\n",
    "n_bits = 4\n",
    "print(\"Binary Counting Examples:\")\n",
    "print(\"=\"*40)\n",
    "for n in range(16):\n",
    "    n_bin = int_to_binary(n, n_bits)\n",
    "    next_bin = int_to_binary((n + 1) % 16, n_bits)\n",
    "    carry_len = get_carry_chain_length(n, n_bits)\n",
    "    print(f\"{n:2d} = {n_bin} → {next_bin} = {(n+1) % 16:2d}  (carry chain: {carry_len})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Carry Chain Length Distribution\n",
    "\n",
    "The carry chain length determines how many bits flip during incrementing:\n",
    "- `k=0`: Only LSB flips (numbers ending in 0)\n",
    "- `k=n-1`: Global carry - all bits flip (e.g., 1111 → 0000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize carry chain distribution (why stratified sampling matters)\n",
    "n_bits = 20\n",
    "carry_counts = [0] * (n_bits + 1)\n",
    "\n",
    "for n in range(2**min(n_bits, 16)):  # Sample first 2^16 for visualization\n",
    "    k = get_carry_chain_length(n, n_bits)\n",
    "    carry_counts[k] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(range(len(carry_counts)), carry_counts, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Carry Chain Length (k)', fontsize=12)\n",
    "ax.set_ylabel('Count (log scale)', fontsize=12)\n",
    "ax.set_title('Natural Distribution of Carry Chain Lengths\\n(Exponential Decay: P(k) = 2^{-k})', fontsize=14)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add theoretical line\n",
    "theoretical = [carry_counts[0] * (0.5**k) for k in range(len(carry_counts))]\n",
    "ax.plot(range(len(carry_counts)), theoretical, 'r--', linewidth=2, label='Theoretical: 2^{-k}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Global carries (k=n-1) are exponentially rare!\")\n",
    "print(\"→ Stratified sampling ensures the model sees all difficulty levels equally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "n_bits = 20\n",
    "train_dataset = LargeCounterDataset(n_bits=n_bits, split='train', train_ratio=0.3, stratified=True)\n",
    "test_dataset = LargeCounterDataset(n_bits=n_bits, split='test', train_ratio=0.3, stratified=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "print(f\"State space size: 2^{n_bits} = {2**n_bits:,}\")\n",
    "print(f\"Training coverage: {len(train_dataset) / 2**n_bits * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample format:\")\n",
    "print(f\"  Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"  Input tokens: {sample['input_ids'][:30].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Training with Grokking <a id='2-training'></a>\n",
    "\n",
    "Key training settings for inducing grokking:\n",
    "- **High weight decay (λ=1.0)**: Critical for \"complexity collapse\"\n",
    "- **Extended training**: Continue well past training convergence\n",
    "- **Small model**: 2 layers, 4 heads, d=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (paper settings)\n",
    "model_config = TransformerConfig(\n",
    "    vocab_size=train_dataset.tokenizer.vocab_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    d_ff=256,\n",
    "    max_seq_len=128,\n",
    "    dropout=0.0,\n",
    "    use_rope=True  # Critical for Same-Bit Lookup\n",
    ")\n",
    "\n",
    "model = TransformerLM(model_config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"Configuration: {model_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "train_config = TrainingConfig(\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1.0,  # High weight decay for grokking!\n",
    "    batch_size=512,\n",
    "    max_steps=10000,  # Reduce for demo (paper uses 50000)\n",
    "    eval_interval=500,\n",
    "    warmup_steps=500,\n",
    "    save_checkpoints=False\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Learning rate: {train_config.learning_rate}\")\n",
    "print(f\"  Weight decay: {train_config.weight_decay} (critical for grokking!)\")\n",
    "print(f\"  Max steps: {train_config.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = create_dataloader(train_dataset, batch_size=train_config.batch_size, shuffle=True)\n",
    "test_loader = create_dataloader(test_dataset, batch_size=train_config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Note: This is a shortened demo. Full training takes ~2 hours.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    config=train_config,\n",
    "    output_dir='../outputs/notebook_demo',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Watch for the grokking phase transition!\")\n",
    "print()\n",
    "\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mechanistic Analysis <a id='3-analysis'></a>\n",
    "\n",
    "After grokking, we analyze the internal circuits to verify alignment with B-RASP theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.attention_analysis import (\n",
    "    analyze_attention_patterns,\n",
    "    compute_diagonal_score,\n",
    "    find_lookup_heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention patterns\n",
    "model.eval()\n",
    "\n",
    "# Generate test sequences\n",
    "test_batch = next(iter(test_loader))\n",
    "input_ids = test_batch['input_ids'][:8].to(device)\n",
    "\n",
    "# Get attention patterns\n",
    "with torch.no_grad():\n",
    "    patterns = model.get_attention_patterns(input_ids)\n",
    "\n",
    "print(f\"Extracted attention patterns from {len(patterns)} layers\")\n",
    "print(f\"Pattern shape: {patterns[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Same-Bit Lookup heads\n",
    "# Theory predicts: attention at offset -(n+1) for retrieving corresponding bit\n",
    "\n",
    "target_offset = -(n_bits + 1)\n",
    "print(f\"Target offset for Same-Bit Lookup: {target_offset}\")\n",
    "print(f\"(For n={n_bits} bits, previous bit is at position -(n+1)={target_offset})\")\n",
    "print()\n",
    "\n",
    "# Compute diagonal scores for each head\n",
    "print(\"Diagonal Attention Scores:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer_idx, layer_attn in enumerate(patterns):\n",
    "    print(f\"\\nLayer {layer_idx}:\")\n",
    "    for head_idx in range(layer_attn.shape[1]):\n",
    "        head_attn = layer_attn[0, head_idx].cpu()\n",
    "        score = compute_diagonal_score(head_attn, offset=target_offset)\n",
    "        bar = '█' * int(score * 20)\n",
    "        print(f\"  Head {head_idx}: {score:.3f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization <a id='4-visualization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.visualization import (\n",
    "    plot_grokking_dynamics,\n",
    "    plot_weight_norm,\n",
    "    plot_attention_heatmap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grokking dynamics (Figure 3 from paper)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy dynamics\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['steps'], history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(history['steps'], history['test_acc'], 'g-', label='Test Accuracy', linewidth=2)\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Training Steps', fontsize=12)\n",
    "ax1.set_ylabel('Sequence Accuracy', fontsize=12)\n",
    "ax1.set_title('Grokking Dynamics: Delayed Generalization', fontsize=14)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight norm (complexity collapse)\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history['steps'], history['weight_norm'], 'purple', linewidth=2)\n",
    "ax2.set_xlabel('Training Steps', fontsize=12)\n",
    "ax2.set_ylabel('L2 Weight Norm ||θ||₂', fontsize=12)\n",
    "ax2.set_title('Mechanism: Complexity Collapse', fontsize=14)\n",
    "ax2.set_xscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"• Train accuracy saturates early (memorization phase)\")\n",
    "print(\"• Test accuracy jumps suddenly (grokking transition)\")\n",
    "print(\"• Weight norm drops sharply at transition (complexity collapse)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention pattern (Figure 4 from paper)\n",
    "# Find the head with highest diagonal score (Same-Bit Lookup head)\n",
    "\n",
    "best_score = 0\n",
    "best_layer, best_head = 0, 0\n",
    "\n",
    "for layer_idx, layer_attn in enumerate(patterns):\n",
    "    for head_idx in range(layer_attn.shape[1]):\n",
    "        score = compute_diagonal_score(layer_attn[0, head_idx].cpu(), offset=target_offset)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_layer, best_head = layer_idx, head_idx\n",
    "\n",
    "print(f\"Best Same-Bit Lookup head: Layer {best_layer}, Head {best_head} (score: {best_score:.3f})\")\n",
    "\n",
    "# Plot the attention pattern\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "attn = patterns[best_layer][0, best_head].cpu().numpy()\n",
    "im = ax.imshow(attn, cmap='Blues', aspect='auto')\n",
    "\n",
    "ax.set_xlabel('Source Position (Input N_i)', fontsize=12)\n",
    "ax.set_ylabel('Target Position (Output N_{i+1})', fontsize=12)\n",
    "ax.set_title(f'Attention Pattern: Layer {best_layer}, Head {best_head}\\n(Same-Bit Lookup at offset {target_offset})', fontsize=14)\n",
    "\n",
    "# Mark the theoretical diagonal\n",
    "for i in range(abs(target_offset), attn.shape[0]):\n",
    "    ax.scatter(i + target_offset, i, marker='o', s=30, c='red', alpha=0.5)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRed dots: Theoretical Same-Bit Lookup positions (offset = -(n+1))\")\n",
    "print(\"High attention along this diagonal confirms the B-RASP circuit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN Baseline Comparison <a id='5-baselines'></a>\n",
    "\n",
    "Compare with RNN baselines to demonstrate the succinctness gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.rnn import compare_model_sizes\n",
    "\n",
    "# Compare model sizes\n",
    "sizes = compare_model_sizes()\n",
    "\n",
    "print(\"Model Parameter Comparison:\")\n",
    "print(\"=\"*50)\n",
    "for name, params in sizes.items():\n",
    "    print(f\"{name:20s}: {params:>10,} parameters\")\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"RNN with 30x more parameters still fails!\")\n",
    "print(\"This is the SUCCINCTNESS GAP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick RNN test (will fail)\n",
    "rnn_config = RNNConfig(\n",
    "    vocab_size=train_dataset.tokenizer.vocab_size,\n",
    "    hidden_dim=2048,  # Even large RNN fails\n",
    "    n_layers=2,\n",
    "    model_type='lstm'\n",
    ")\n",
    "\n",
    "rnn_model = RNNLM(rnn_config).to(device)\n",
    "rnn_params = sum(p.numel() for p in rnn_model.parameters())\n",
    "\n",
    "print(f\"LSTM Parameters: {rnn_params:,}\")\n",
    "print(f\"Transformer Parameters: {n_params:,}\")\n",
    "print(f\"RNN is {rnn_params/n_params:.1f}x larger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the succinctness gap (Figure 8 from paper)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Transformer\\n(d=64)', 'LSTM\\n(d=64)', 'LSTM\\n(d=256)', 'LSTM\\n(d=1024)', 'LSTM\\n(d=2048)', 'GRU\\n(d=2048)']\n",
    "accuracies = [100.0, 0.0, 0.0, 4.2, 5.8, 5.8]  # Paper results\n",
    "colors = ['green'] + ['red'] * 5\n",
    "\n",
    "bars = ax.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Annotate\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.annotate(f'{acc:.1f}%', \n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add note box\n",
    "ax.annotate('Comparison on n=20 bits\\nRNNs fail to generalize\\neven with 30x parameters', \n",
    "            xy=(0.7, 0.7), xycoords='axes fraction',\n",
    "            fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax.set_ylabel('Sequence-Level Test Accuracy (%)', fontsize=12)\n",
    "ax.set_title('The Succinctness Gap: Transformers vs RNNs', fontsize=14)\n",
    "ax.set_ylim(0, 110)\n",
    "ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Succinctness Gap**: Transformers (50K params) achieve 100% accuracy while RNNs (1.5M params) fail (<6%)\n",
    "\n",
    "2. **Grokking Phase Transition**: The succinct circuit emerges via:\n",
    "   - Memorization phase (high weight norm)\n",
    "   - Complexity collapse (weight norm drops)\n",
    "   - Generalization phase (100% test accuracy)\n",
    "\n",
    "3. **Mechanistic Alignment with B-RASP**:\n",
    "   - Same-Bit Lookup heads with precise offset -(n+1)\n",
    "   - MLPs implementing XOR/AND logic\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you use this code, please cite:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{anonymous2026grokking,\n",
    "    title={Do Transformers Grok Succinct Algorithms? Mechanistic Evidence for Counting Circuits},\n",
    "    author={Anonymous},\n",
    "    booktitle={Proceedings of ACL 2026},\n",
    "    year={2026}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
